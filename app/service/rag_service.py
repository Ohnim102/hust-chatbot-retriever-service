from typing import List, Dict
import os
from fastapi import Depends, UploadFile
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredPDFLoader,
    Docx2txtLoader,
    UnstructuredHTMLLoader,
    UnstructuredExcelLoader,
)
# from pdf2image import convert_from_path

import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import io
from pdfminer.high_level import extract_text

# from docling.parsers import PdfParser, WordParser, HtmlParser, ExcelParser
# from docling.document_converter import DocumentConverter

from langchain_text_splitters import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
)
from app.models.document import Document
# from app.rag.chromadb import ChromaDB
from app.rag.qdrantdb import QdrantDB
import re
import unicodedata
from app.transformers.rag_file_transformer import transform_documents
from app.transformers.rag_content_transformer import transform_to_content
from app.setting.enum import DocsCollection
from app.models.prompt import OllamaPrompt, OllamaMessage
import platform


class RAGService:
    def __init__(self):
        self.chatid = 1;

    def clean_text(self, text: str) -> str:
        return text.strip()

    def is_meaningful(self, text: str) -> bool:
        if re.search(r"(g c√á|H¬£¬Ø|Dxcl|Tl√ë¬°|p·ªá|√ù√é|¬Øl·∫ß|ƒê·∫ß√á|g l|gI|√Åx|√Å¬≤|√Å√†|√å)", text):
            return False
        count_alpha = sum(c.isalpha() for c in text)
        count_visible = sum(1 for c in text if c.isprintable())
        if count_visible == 0 or (count_alpha / count_visible) < 0.5:
            return False
        count_weird = sum(
            1 for c in text if not re.match(r"[a-zA-Z√Ä-·ªπ√†-·ªπ0-9\s.,!?\"'‚Äô\-‚Äì()]", c)
        )
        if count_weird / len(text) > 0.2:
            return False
        count_control = sum(1 for c in text if unicodedata.category(c).startswith("C"))
        if count_control > 0:
            return False
        if re.fullmatch(r"^[\W\d\s]+$", text.strip()):
            return False

        return True

    # Clean documents before embedding
    async def clean_documents(self, documents: List[Document]) -> List[Document]:
        cleaned_docs = []
        for doc in documents:
            text = doc.page_content
            # Except for the first document which is the file name
            if not text.startswith("T√™n file"):
                # 1.Clean page_content
                text = text.replace("\r\n", " ").replace("\n", " ").replace("\t", " ")
                text = re.sub(r"[^\x20-\x7E√Ä-·ªπ\u00A0-\uFFFF]", "", text)
                text = re.sub(r"\s+", " ", text)
                text = re.sub(r"[.,!?;:]+", ".", text)
                text = re.sub(r"^[.,!?;:]+", "", text)
                text = re.sub(r"[.,!?;:]+$", ".", text)
                text = text.strip()

                if len(text.strip().split()) < 6:
                    continue
                if re.fullmatch(r"[\W\d\s]+", text):
                    continue
                # if not self.is_meaningful(text):
                #     continue
            doc.page_content = text
            print(f"Content doc after cleaning: {text}")

            # 2.Clean metadata
            if doc.metadata is not None:
                metadata_fields_to_remove = [
                    "moddate",
                    "creator",
                    "producer",
                    "page_label",
                ]
                for field in metadata_fields_to_remove:
                    if field in doc.metadata:
                        del doc.metadata[field]

            cleaned_docs.append(doc)

        return cleaned_docs

    def add_file_name_to_start(
        self, metadata, documents: List[Document]
    ) -> List[Document]:
        source_path = metadata.get("source", "")
        file_name = os.path.basename(source_path)
        if not file_name:
            file_name = "Untitled"

        file_doc = Document(page_content="T√™n file: " + file_name, metadata=metadata)
        documents.insert(0, file_doc)

        return documents
    
    # pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

    if platform.system() == "Windows":
        pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
    else:
        pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

    @staticmethod
    def pdf_to_text_ocr_fitz(file_path: str, lang: str = "vie") -> str:
        """
        ƒê·ªçc file PDF scan (image-based) b·∫±ng PyMuPDF v√† Tesseract OCR.
        Kh√¥ng c·∫ßn Poppler.
        """
        text_result = ""
        doc = fitz.open(file_path)
        print(f"üîç Found {doc.page_count} pages in {file_path}")

        for page_index in range(doc.page_count):
            page = doc.load_page(page_index)
            pix = page.get_pixmap(dpi=300)  # render v·ªõi ƒë·ªô ph√¢n gi·∫£i cao
            mode = "RGBA" if pix.alpha else "RGB"
            img = Image.frombytes(mode, [pix.width, pix.height], pix.samples)

            # OCR
            text = pytesseract.image_to_string(img, lang=lang)
            text_result += f"\n=== Page {page_index + 1} ===\n{text}\n"

        return text_result
    
    # Load and split document
    async def load_and_split_document(
        self,
        doc_id: str,
        file_path: str,
        collection_name: DocsCollection,
        options: Dict[str, any] = {
            "chunk_size": 120,
            "chunk_overlap": 0,
        },
    ) -> str:
        # TODO: Handle file pdf with UnstructuredPDFLoader
        ext = os.path.splitext(file_path)[1].lower()
        documents: List[Document] = []

        ## --- Using Langchain loaders ---
        # if file_path.endswith(".pdf"):
        #     loader = PyPDFLoader(file_path)
        # elif file_path.endswith(".docx"):
        #     loader = Docx2txtLoader(file_path)
        # elif file_path.endswith(".xlsx"):
        #     loader = UnstructuredExcelLoader(file_path)
        # elif file_path.endswith(".html"):
        #     loader = UnstructuredHTMLLoader(file_path)
        # else:
        #     raise ValueError(f"Unsupported file type: {file_path}")
        # documents = loader.load()

        ## --- Using Docling library (not maintained) ---
        # converter = DocumentConverter()
        # result = converter.convert(file_path)
        # text = result.document.export_to_text()
        # documents = [Document(page_content=text, metadata={"source": file_path})]

        ## --- Using UnstructuredPDFLoader with OCR fallback ---
        if ext == ".pdf":
            try:
                text = extract_text(file_path)
                if text and len(text.strip()) > 20:
                    print("üìÑ PDF has text layer ‚Äî using PyPDFLoader")
                    loader = PyPDFLoader(file_path)
                    documents = loader.load()
                else:
                    full_text = RAGService.pdf_to_text_ocr_fitz(file_path, lang="vie")
                    full_text = self.pdf_to_text_ocr_fitz(file_path, lang="vie")
                    documents = [Document(page_content=full_text, metadata={"source": file_path})]
            except Exception as e:
                print(f"‚ö†Ô∏è OCR fallback failed: {e}")
                raise ValueError(f"OCR fallback failed: {file_path}")
        elif ext == ".docx":
            loader = Docx2txtLoader(file_path)
            documents = loader.load()
        elif ext in [".xlsx", ".xls"]:
            loader = UnstructuredExcelLoader(file_path)
            documents = loader.load()
        elif ext in [".html", ".htm"]:
            loader = UnstructuredHTMLLoader(file_path)
            documents = loader.load()
        elif ext == ".txt":
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
                documents = [Document(page_content=text, metadata={"source": file_path})]
        else:
            raise ValueError(f"Unsupported file type: {file_path}")        

        for doc in documents:
            doc.page_content = self.clean_text(doc.page_content)

        # Split documents into smaller chunks
        if documents:
            documents = self.split_documents(documents, options)

        documents_add_page_name = self.add_file_name_to_start(documents[0].metadata, documents)
        documents_cleaned = await self.clean_documents(documents_add_page_name)
        await self.add_to_vector_db(doc_id, documents_cleaned, collection_name)

        return file_path

    # Query document from VectorDB
    async def query_document(
        self, collection_name: DocsCollection, query: str, k: int = 5
    ):
        vectordb_instance = QdrantDB(collection_name=collection_name)
        documents = vectordb_instance.query(query, k)

        return transform_documents(documents)

    async def query_rag_content_document(
        self, collection_name: DocsCollection, query: str, k: int = 5
    ):
        vectordb_instance = QdrantDB(collection_name=collection_name)
        documents = vectordb_instance.query(query, k)

        return transform_to_content(documents)

    async def add_to_vector_db(
        self, doc_id: str, documents: List[Document], collection_name: DocsCollection
    ) -> List[Document]:
        vectordb_instance = QdrantDB(collection_name=collection_name)
        await vectordb_instance.add_documents(doc_id, documents)

        return True

    async def delete_documents_by_doc_id(
            self, doc_id: str, collection_name: DocsCollection
    ):
        vectordb_instance = QdrantDB(collection_name=collection_name)
        vectordb_instance.delete_documents_by_doc_id(doc_id)

        return True
        

    async def clear_vectordb(self, collection_name: DocsCollection) -> bool:
        vectordb_instance = QdrantDB(collection_name=collection_name)
        return vectordb_instance.delete_collection()

    def split_documents(
        self,
        documents: List[Document],
        options: Dict[str, any] = {
            "chunk_size": 120,
            "chunk_overlap": 0,
        },
    ) -> List[Document]:
        # Split document by priority level
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=options.get("chunk_size", 120),
            chunk_overlap=options.get("chunk_overlap", 0),
            separators=["\n\n", "\n", ". ", "! ", "? ", "+ ", "- ", " ", ""],
        )

        texts = text_splitter.split_documents(documents)
        return texts

    async def generate_prompt(self, 
                              query: str, 
                              collection: DocsCollection = DocsCollection.SEARCH
                              ) -> List[Dict[str, str]]:
        """
        Generate a prompt for the RAG model using the query and context documents.

        Args:
            query (str): The user's query.

        Returns:
            OllamaPrompt: The generated prompt.
        """
        # L·∫•y ng·ªØ c·∫£nh t·ª´ c∆° s·ªü d·ªØ li·ªáu Vector DB
        context_documents = await self.query_document(
            collection, query, k=5
        )

        # N·∫øu kh√¥ng c√≥ t√†i li·ªáu ng·ªØ c·∫£nh, tr·∫£ v·ªÅ th√¥ng b√°o kh√¥ng c√≥ th√¥ng tin
        if not context_documents:
            context_documents = "Kh√¥ng c√≥ th√¥ng tin n√†o ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."

        prompt_messages = [
            dict(
                role="user",
                content=f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI c√≥ t√™n l√† a.Guide thu·ªôc tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch Khoa H√† N·ªôi. 
                Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ti·∫øng vi·ªát d·ª±a tr√™n c√°c t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c cung c·∫•p. 
                H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ng·∫Øn g·ªçn, s√∫c t√≠ch v√† r√µ r√†ng, tr√≠ch ra ngu·ªìn c·ªßa t√†i li·ªáu n·∫øu c√≥ th·ªÉ. 
                N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn t√†i li·ªáu, b·∫°n c√≥ th·ªÉ d√πng ki·∫øn th·ª©c c·ªßa m√¨nh ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
                N·∫øu c√¢u h·ªèi kh√¥ng r√µ r√†ng ho·∫∑c kh√¥ng ƒë·∫ßy ƒë·ªß, h√£y y√™u c·∫ßu ng∆∞·ªùi d√πng cung c·∫•p th√™m th√¥ng tin. 
                C√¢u h·ªèi nh∆∞ sau: \n{query}""",
            ),
            dict(role="user", content=f"T√†i li·ªáu:\n{context_documents}"),
        ]

        return prompt_messages


def get_rag_service() -> RAGService:
    return RAGService()
